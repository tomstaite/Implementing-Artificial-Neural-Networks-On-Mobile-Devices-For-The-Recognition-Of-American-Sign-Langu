LINK TO PROJECT REPORT: https://drive.google.com/file/d/1ke-qbqlMMGZTeCqVDwAc9Rnnve_X4lQm/view

Project Title: Implementing Artificial Neural Networks On Mobile Devices For The Recognition Of American Sign Language (ASL) 

Project Description:

A significant proportion of my overall degree classification came from the 'Final Year Project'. This gave students the opportunity to complete an extended piece of work in a particular field of Computer Science that they were interested in. For me personally, it was a no-brainer - I had to choose Artifical Intelligence.

With that being said, I met with my project supervisor shortly after selecting my area of interest and discussed a few project ideas that I had proposed. We conversed about the feasibility of the ideas; given the time and resource constraints, and eventually agreed on a project that I could undertake.

The selected project consisted of research on how Neural Networks are used to classify a range of images, along with looking at the different types of NN that currently exist and when it is suitable to use each type. With this knowledge at my disposal, I proceeded to identify an existing Neural Networks for the recognition of ASL (American Sign Language) gestures and trained it with a dataset of over 70,000 images.

The fully trained NN was adapted in various ways in an attempt to boost prediction accuracy. The final level of accuracy that was achieved was 97.7% which was an increase of 29.7% from the original training iteration.

The resulting NN was then implemented into an iOS Application that was built to serve the purpose of teaching ASL. I used Apple's CoreML tools to convert the NN into the required format and tested it with a number of images to confirm that it could still classify the images correctly.
